# 结果一致性问题说明

## 问题现象

相同的简历数据,两种运行方式得到不同的结果:

| 运行方式 | 简历2(张明) | 说明 |
|---------|-----------|------|
| 7.LLM_resume_filter直接运行 | ✅ 通过 | "博士学历,系统内工作3年,符合放宽至2年的规定" |
| backend.py Web接口 | ❌ 不通过 | "系统内工作时长不足2年" (错误判断) |

**实际数据**: 简历2的"系统内工作时长(年)": 3 → 应该通过

## 根本原因

### LLM的随机性 (temperature=0.7)

在 `7.LLM_resume_filter/managers/llm_manager.py` 中:

```python
self.llm = ChatOpenAI(
    temperature=0.7,  # ⚠️ 导致输出随机性
    ...
)
```

**影响**: 相同输入可能产生不同输出,导致:
- 有时判断"系统内工作3年" → 通过 ✅
- 有时判断"系统内工作不足2年" → 不通过 ❌ (错误!)

## 解决方案

### ✅ 已修改

1. **设置 temperature=0** (确保结果一致性)
   ```python
   self.llm = ChatOpenAI(
       temperature=0,  # ✅ 相同输入始终产生相同输出
       ...
   )
   ```

2. **backend.py直接解析Excel** (不依赖外部JSON)
   - 之前: 依赖 `7.LLM_resume_filter/data/xxx_去掉系统外.json`
   - 现在: 直接解析上传的Excel文件

## 验证

重启服务后测试:

```bash
# 停止旧服务
lsof -ti:8000 | xargs kill -9 2>/dev/null

# 启动新服务
cd "5.界面/条件较为简单+多行表"
python3 backend.py
```

## 预期效果

修改后,无论运行多少次,相同的输入应该始终产生相同的输出。

## 注意

- temperature=0 会让输出更"机械",但对于简历筛选这种需要一致性的场景是正确的选择
- LLM判断仍可能出错,建议增加人工复核环节
