# from langchain_core.prompts import ChatPromptTemplate
# from langchain_openai import ChatOpenAI
# from langchain_core.output_parsers import StrOutputParser

# # 1. å®šä¹‰æç¤ºè¯æ¨¡æ¿
# prompt = ChatPromptTemplate.from_template(
#     "ä»Šå¤©{city}çš„å¤©æ°”æ˜¯{weather}ï¼Œè¯·ç»™ç”¨æˆ·ä¸€æ¡ç©¿è¡£å»ºè®®ã€‚"
# )

# # 2. ä½¿ç”¨ OpenAI æ¨¡å‹
# model = ChatOpenAI(
#     model="qwq-32b",  # é˜¿é‡Œäº‘ DashScope çš„æ¨¡å‹å
#     api_key="sk-dff6d0f5d956485fa0ac71998e63d036",  # æ›¿æ¢ä¸ºä½ çš„ DashScope API Key
#     base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
# )

# # 3. ç”¨ LCEL æ‹¼æ¥ï¼šè¾“å…¥ â†’ æç¤º â†’ æ¨¡å‹ â†’ è§£æä¸ºå­—ç¬¦ä¸²
# chain = prompt | model | StrOutputParser()

# # # 4. è°ƒç”¨ï¼ˆåŒæ­¥ï¼‰
# # result = chain.invoke({"city": "åŒ—äº¬", "weather": "æ™´ï¼Œ15Â°C"})
# # print(result)  # è¾“å‡ºï¼š"å»ºè®®ç©¿é•¿è¢–è¡¬è¡«åŠ è–„å¤–å¥—..."

# # 5. æµå¼è¾“å‡ºï¼ˆç”¨æˆ·ä½“éªŒæ›´å¥½ï¼ï¼‰
# for chunk in chain.stream({"city": "ä¸Šæµ·", "weather": "é›¨ï¼Œ10Â°C"}):
#     print(chunk, end="", flush=True)

# test_self_rag_agent.py
from typing import TypedDict, List, Literal
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# ===== æ¨¡æ‹Ÿæ£€ç´¢å™¨ï¼ˆæ›¿æ¢æˆä½ çš„å‘é‡åº“ï¼‰=====
def fake_retriever(query: str) -> List[str]:
    # ç®€å•è§„åˆ™æ¨¡æ‹Ÿï¼šæ ¹æ®å…³é”®è¯è¿”å›â€œæ–‡æ¡£â€
    if "å·´é»" in query:
        return ["å·´é»æ˜¯æ³•å›½é¦–éƒ½ï¼Œè‘—åæ™¯ç‚¹æœ‰åŸƒè²å°”é“å¡”ã€å¢æµ®å®«ã€‚"]
    elif "ä¸œäº¬" in query:
        return ["ä¸œäº¬æ˜¯æ—¥æœ¬é¦–éƒ½ï¼Œä»¥æ¶©è°·ã€æµ…è‰å¯ºé—»åã€‚"]
    else:
        return ["æœªçŸ¥åœ°ç‚¹ä¿¡æ¯ã€‚"]

# ===== LLM è®¾ç½® =====
llm = ChatOpenAI(
    model="qwen-max",  # é˜¿é‡Œäº‘ DashScope çš„æ¨¡å‹å
    api_key="sk-dff6d0f5d956485fa0ac71998e63d036",  # æ›¿æ¢ä¸ºä½ çš„ DashScope API Key
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)


# ===== å®šä¹‰ Agent çŠ¶æ€ =====
class AgentState(TypedDict):
    query: str
    retrieved_docs: List[str]
    draft_answer: str
    reflection: str
    should_retry: bool
    final_answer: str
    retry_count: int

# ===== èŠ‚ç‚¹å‡½æ•° =====
def retrieve(state: AgentState):
    docs = fake_retriever(state["query"])
    return {"retrieved_docs": docs, "retry_count": state.get("retry_count", 0) + 1}

def generate_draft(state: AgentState):
    prompt = ChatPromptTemplate.from_template(
        "åŸºäºä»¥ä¸‹æ–‡æ¡£å›ç­”é—®é¢˜ï¼š\n\næ–‡æ¡£ï¼š{docs}\n\né—®é¢˜ï¼š{query}\n\nç­”æ¡ˆï¼š"
    )
    chain = prompt | llm | StrOutputParser()
    answer = chain.invoke({"docs": "\n".join(state["retrieved_docs"]), "query": state["query"]})
    return {"draft_answer": answer}

def self_reflect(state: AgentState):
    reflect_prompt = ChatPromptTemplate.from_template(
        "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„åŠ©æ‰‹ã€‚è¯·è¯„ä¼°ä»¥ä¸‹ç­”æ¡ˆæ˜¯å¦å¯é ã€æ˜¯å¦å®Œå…¨åŸºäºæä¾›çš„æ–‡æ¡£ã€‚\n\n"
        "é—®é¢˜ï¼š{query}\n"
        "æ–‡æ¡£ï¼š{docs}\n"
        "å½“å‰ç­”æ¡ˆï¼š{answer}\n\n"
        "è¯·å›ç­”ï¼šè¿™ä¸ªç­”æ¡ˆæ˜¯å¦å¯ä¿¡ï¼Ÿæ˜¯å¦å­˜åœ¨å¹»è§‰æˆ–ä¿¡æ¯ä¸è¶³ï¼Ÿæ˜¯å¦éœ€è¦é‡æ–°æ£€ç´¢ï¼Ÿ\n"
        "ä»…å›ç­” 'éœ€è¦é‡è¯•' æˆ– 'æ— éœ€é‡è¯•'ã€‚"
    )
    chain = reflect_prompt | llm | StrOutputParser()
    reflection = chain.invoke({
        "query": state["query"],
        "docs": "\n".join(state["retrieved_docs"]),
        "answer": state["draft_answer"]
    })
    should_retry = "éœ€è¦é‡è¯•" in reflection
    return {"reflection": reflection, "should_retry": should_retry}

def finalize(state: AgentState):
    return {"final_answer": state["draft_answer"]}

# ===== æ¡ä»¶è·¯ç”±å‡½æ•° =====
def route_after_reflection(state: AgentState) -> Literal["retrieve", "finalize"]:
    if state["should_retry"] and state["retry_count"] < 2:
        return "retrieve"
    else:
        return "finalize"

# ===== æ„å»º LangGraph =====
workflow = StateGraph(AgentState)

workflow.add_node("retrieve", retrieve)
workflow.add_node("generate_draft", generate_draft)
workflow.add_node("self_reflect", self_reflect)
workflow.add_node("finalize", finalize)

workflow.set_entry_point("retrieve")
workflow.add_edge("retrieve", "generate_draft")
workflow.add_edge("generate_draft", "self_reflect")
workflow.add_conditional_edges("self_reflect", route_after_reflection)
workflow.add_edge("finalize", END)

app = workflow.compile()

# ===== æµ‹è¯•è¿è¡Œ =====
if __name__ == "__main__":
    import os
    os.environ["OPENAI_API_KEY"] = "sk-dff6d0f5d956485fa0ac71998e63d036"  # æ›¿æ¢ä¸ºä½ çš„å¯†é’¥

    inputs = {"query": "å·´é»æœ‰å“ªäº›è‘—åæ™¯ç‚¹ï¼Ÿ"}
    result = app.invoke(inputs)
    print("\nâœ… æœ€ç»ˆç­”æ¡ˆï¼š")
    print(result["final_answer"])
    print("\nğŸ” åæ€è®°å½•ï¼š", result["reflection"])
    print("ğŸ”„ é‡è¯•æ¬¡æ•°ï¼š", result["retry_count"])